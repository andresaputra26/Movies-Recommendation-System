# -*- coding: utf-8 -*-
"""Movie Recommendation System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18VyUO_5t3tD9_YPd62H7L2y54g_Wi7R6

# Import Library
"""

!pip install opendatasets
import pandas as pd
import numpy as np
import opendatasets as od
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

"""# Load Dataset"""

od.download("https://www.kaggle.com/datasets/nicoletacilibiu/movies-and-ratings-for-recommendation-system?select=movies.csv")

movies = pd.read_csv("/content/movies-and-ratings-for-recommendation-system/movies.csv")
ratings = pd.read_csv("/content/movies-and-ratings-for-recommendation-system/ratings.csv")

"""# Data Understanding

## Movies Dataset
"""

movies.head()

# List untuk menyimpan ringkasan per kolom
list_of_descs = []

# Menggunakan dataframe 'movies' sebagai contoh, ganti dengan 'ratings' jika perlu
for col in movies.columns:
    col_type = movies[col].dtype
    total_row = movies.shape[0]
    null_count = movies[col].isna().sum()
    null_pct = round((null_count / total_row) * 100, 2)
    unique_count = movies[col].nunique()
    sample_unique = movies[col].unique()

    list_of_descs.append([
        col,
        col_type,
        total_row,
        null_count,
        null_pct,
        unique_count,
        sample_unique
    ])

# Buat dataframe ringkasan
df_desc = pd.DataFrame(
    list_of_descs,
    columns=[
        'Column',
        'Type',
        'Total Rows',
        'Null Count',
        'Null %',
        'Unique Values',
        'Sample Unique Values'
    ]
)

# Tampilkan ringkasan
df_desc

"""### üìù Ringkasan Dataset `movies.csv`

* **movieId** (`int64`)
  ‚Üí ID unik film, total 9742, **tidak ada duplikat atau null**.

* **title** (`object`)
  ‚Üí Judul film, ada **5 duplikat judul** (9737 unik dari 9742), **tidak ada null**.

* **genres** (`object`)
  ‚Üí Gabungan genre (dipisah `|`), total **951 kombinasi unik**, **tidak ada null**.

"""

# Cek duplikasi pada kolom 'title' di dataframe 'movies'
duplicate_titles = movies[movies.duplicated(subset=['title'], keep=False)]

# Tampilkan baris yang mengandung duplikasi pada kolom 'title'
if not duplicate_titles.empty:
  print("Baris dengan judul duplikat:")
  print(duplicate_titles.sort_values(by='title'))
else:
  print("Tidak ada judul duplikat dalam dataset movies.")

"""Terlihat bahwa Judulnya sama namun genrenya berbeda jadi tidak 100% duplikat

## Ratings Dataset
"""

ratings.head()

# List untuk menyimpan ringkasan per kolom
list_of_descs = []

# Menggunakan dataframe 'ratings'
for col in ratings.columns:
    col_type = ratings[col].dtype
    total_row = ratings.shape[0]
    null_count = ratings[col].isna().sum()
    null_pct = round((null_count / total_row) * 100, 2)
    unique_count = ratings[col].nunique()
    sample_unique = ratings[col].unique()

    list_of_descs.append([
        col,
        col_type,
        total_row,
        null_count,
        null_pct,
        unique_count,
        sample_unique
    ])

# Buat dataframe ringkasan
df_desc = pd.DataFrame(
    list_of_descs,
    columns=[
        'Column',
        'Type',
        'Total Rows',
        'Null Count',
        'Null %',
        'Unique Values',
        'Sample Unique Values'
    ]
)

# Tampilkan ringkasan
df_desc

"""### üìù Ringkasan Dataset `ratings.csv`

* **userId** (`int64`)
  ‚Üí ID unik pengguna, total 100,836 baris, **610 user unik**, **tidak ada null**.

* **movieId** (`int64`)
  ‚Üí ID unik film, total 100,836 baris, **9,724 film unik**, **tidak ada null**.

* **rating** (`float64`)
  ‚Üí Nilai rating film (skala 0.5‚Äì5.0), total 10 nilai berbeda, **tidak ada null**.

* **timestamp** (`int64`)
  ‚Üí Waktu rating dalam format epoch, total 85,043 nilai unik, **tidak ada null**.

# Data Preprocessing

## Merge Data `movies` dan `ratings`
"""

df_merge = pd.merge(movies, ratings, on='movieId',  how='left')
df_merge.head()

"""## Menghapus kolom `timestamp`"""

df_merge = df_merge.drop('timestamp', axis=1)
df_merge.head()

"""menghapus kolom `timestamp` karena tidak digunakan

## Mengatasi Missing Values
"""

df_merge.isna().sum()

"""Data tersebut menunjukkan adanya **missing values** atau nilai yang hilang di beberapa kolom, yaitu:

* Kolom `userId` memiliki 18 nilai kosong.
* Kolom `rating` juga memiliki 18 nilai kosong.
* Sedangkan kolom `movieId`, `title`, dan `genres` tidak memiliki nilai kosong sama sekali.

### Menghapus Missing Values
"""

# Drop rows with missing values
df_merge.dropna(inplace=True)

# Verify that missing values have been removed
df_merge.isna().sum()

"""## Mengatasi data duplikat"""

print(f'Jumlah data duplikat: ', df_merge.duplicated().sum())

"""## Menghapus Simbol pada kolom `title` dan `genres`"""

df_merge['title'] = df_merge['title'].str.replace(r"\((\d{4})\)", r"\1", regex=True)
df_merge['genres'] = df_merge['genres'].str.split('|')

print(df_merge.head())

"""menghapus simbol `|` agar bisa digunakan untuk tahap selanjutnya"""

# Membuat list untuk menyimpan genre yang unik
all_genres = []

# Iterasi melalui setiap baris dataframe
for index, row in df_merge.iterrows():
    # Periksa apakah nilai di kolom 'genres' adalah list
    if isinstance(row['genres'], list):
        # Tambahkan setiap genre dalam list ke list all_genres
        all_genres.extend(row['genres'])
    else:
        if pd.notna(row['genres']): # Pastikan bukan NaN
            all_genres.append(row['genres'])


# Mengubah list menjadi set untuk mendapatkan genre yang unik
unique_genres = set(all_genres)

# Menampilkan jenis unique genres
print("Jenis unique genres:")
for genre in unique_genres:
    print(genre)

print(f'\nJumlah unique genres:', len(unique_genres))

"""### Menghapus data pada kolom `title` yang tidak memiliki genre"""

df_merge = df_merge[df_merge['genres'].apply(lambda x: '(no genres listed)' not in x)]

print(f'Jumlah baris setelah menghapus data dengan "no genres listed":', len(df_merge))
print(df_merge.head())

print("\nJumlah nilai unique pada kolom 'rating' dan jumlahnya:")
print(ratings['rating'].value_counts())

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Explode kolom genres agar satu genre per baris
genres_exploded = df_merge.explode('genres')

# Hitung jumlah film per genre
genre_counts = genres_exploded['genres'].value_counts()

# Ambil top 20 genre
top_genres = genre_counts.head(20)
total_count = genre_counts.sum()  # total semua genre (bisa lebih dari jumlah film karena multi-genre)

# Buat DataFrame
top_genres_df = top_genres.reset_index()
top_genres_df.columns = ['Genre', 'Count']

# Plot
plt.figure(figsize=(12, 8))
ax = sns.barplot(data=top_genres_df, x='Genre', y='Count', hue='Genre', palette='viridis', legend=False)

# Tambahkan label jumlah dan persentase di atas batang
for i, row in top_genres_df.iterrows():
    count = row['Count']
    percent = (count / total_count) * 100
    ax.text(i, count + 1, f'{count}\n({percent:.1f}%)', ha='center', va='bottom', fontsize=9)

# Styling
plt.title('Jumlah Judul Film Berdasarkan Genre')
plt.xlabel('Genre')
plt.ylabel('Jumlah Judul Film')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

"""Hasil Visualisasi


* **Genre Drama** merupakan genre yang paling banyak muncul, dengan **41.928 judul film** atau sekitar **15,3%** dari total keseluruhan.
* Diikuti oleh **Comedy (14,2%)**, **Action (11,2%)**, dan **Thriller (9,6%)**, yang juga memiliki jumlah film yang cukup tinggi.
* Genre seperti **Western, Documentary, dan Film-Noir** memiliki jumlah film yang jauh lebih sedikit, masing-masing hanya **0,7%**, **0,4%**, dan **0,3%** dari total.
"""

# Visualisasi Distribusi Rating
plt.figure(figsize=(10, 6))
sns.histplot(df_merge['rating'], bins=10, kde=True)
plt.title('Distribusi Rating Film')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.xticks(np.arange(0.5, 5.5, 0.5)) # Set ticks every 0.5 unit
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

"""Hasil Visualisasi

* Rating paling banyak diberikan adalah **4.0**, disusul oleh **3.5 dan 5.0**, yang artinya sebagian besar pengguna memberikan rating positif terhadap film.
* Jumlah rating yang sangat rendah seperti **0.5 hingga 1.5** relatif jarang diberikan.
* Pola distribusinya agak **condong ke kanan (positif skewed)**, menunjukkan kecenderungan pengguna untuk memberi rating tinggi daripada rendah.
"""

# Hitung jumlah rating per judul film
rating_counts_per_title = df_merge['title'].value_counts()

# Ambil 10 judul dengan rating terbanyak
top_10_rated_titles = rating_counts_per_title.head(10)

# Buat DataFrame dari top 10 judul
top_10_rated_titles_df = top_10_rated_titles.reset_index()
top_10_rated_titles_df.columns = ['Title', 'Number of Ratings']

# Visualisasi
plt.figure(figsize=(14, 8))
sns.barplot(data=top_10_rated_titles_df, x='Number of Ratings', y='Title', hue='Title', palette='viridis', legend=False)
plt.title('Top 10 Judul Film dengan Rating Terbanyak')
plt.xlabel('Jumlah Rating')
plt.ylabel('Judul Film')
plt.tight_layout()
plt.show()

"""Hasil Visualisasi:

* **Forrest Gump (1994)** menjadi film dengan **jumlah rating terbanyak**, disusul oleh **The Shawshank Redemption (1994)** dan **Pulp Fiction (1994)**.
* Film-film yang masuk dalam daftar ini sebagian besar merupakan **film klasik populer dari tahun 1990-an**, menunjukkan bahwa film-film era tersebut sangat populer dan banyak ditonton.
* Semua film yang ditampilkan memiliki jumlah rating di atas 200, menunjukkan tingkat keterlibatan pengguna yang tinggi terhadap judul-judul tersebut.

"""

# Visualisasi Rata-rata Rating per Genre (Semua Genre)
# Hitung rata-rata rating per genre
average_rating_per_genre = genres_exploded.groupby('genres')['rating'].mean().sort_values(ascending=False)

# Konversi ke DataFrame untuk visualisasi
average_rating_df = average_rating_per_genre.reset_index()
average_rating_df.columns = ['Genre', 'Average Rating']

plt.figure(figsize=(14, 10)) # Adjust figure size for more genres
sns.barplot(data=average_rating_df, x='Average Rating', y='Genre', hue='Genre', palette='viridis', legend=False)
plt.title('Rata-rata Rating per Genre (Semua Genre)')
plt.xlabel('Rata-rata Rating')
plt.ylabel('Genre')
# Optional: Adjust x-axis limits if needed
plt.xlim(average_rating_df['Average Rating'].min() * 0.9, average_rating_df['Average Rating'].max() * 1.1)
plt.tight_layout()
plt.show()

"""Hasil Visualisasi:

1. **Genre dengan Rata-Rata Rating Tertinggi**:

   * **Film-Noir** memiliki rating rata-rata tertinggi, di atas 4.0, menunjukkan bahwa meskipun mungkin tidak banyak film bergenre ini, kualitasnya dinilai sangat baik oleh penonton.
   * Diikuti oleh **War** dan **Documentary**, yang juga memiliki rating tinggi ‚Äî ini bisa menunjukkan preferensi penonton terhadap film dengan nilai sejarah, realisme, atau konten mendalam.

2. **Genre Populer dengan Rating Sedang**:

   * Genre seperti **Drama**, **Crime**, **Mystery**, dan **Animation** memiliki rating rata-rata yang baik (sekitar 3.6‚Äì3.7), yang menunjukkan kualitas rata-rata film cukup tinggi dan sering diminati.

3. **Genre dengan Rata-Rata Rating Terendah**:

   * **Horror** berada di posisi terbawah dengan rating rata-rata terendah (sekitar 3.3), disusul oleh **Comedy** dan **Children**.
   * Hal ini bisa mengindikasikan bahwa film horor dan komedi cenderung lebih bervariasi dalam kualitas (ada yang bagus, banyak juga yang dinilai rendah), atau mungkin tidak semua penonton menyukai genre tersebut.

"""

df_merge

"""## Content Based Filtering"""

df_cbf = df_merge[['movieId', 'title', 'genres']]
df_cbf.head()

"""### Prepocessing Data for Content Based Filtering"""

# Convert the list of genres back to a string representation for hashing
df_cbf['genres_str'] = df_cbf['genres'].apply(lambda x: '|'.join(x))

# Drop duplicates using the new string column and the title column
df_cbf = df_cbf.drop_duplicates(subset=['title', 'genres_str'])

# You can drop the temporary 'genres_str' column if you no longer need it
df_cbf = df_cbf.drop('genres_str', axis=1)

print("Jumlah baris setelah menghapus duplikat:", len(df_cbf))
df_cbf.head()

"""### TF-IDF Vectorizer"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Convert the list of genres into a single string separated by spaces
# This makes the 'genres' column suitable for TfidfVectorizer
df_cbf['genres_string'] = df_cbf['genres'].apply(lambda x: ' '.join(x))

# Fit the TfidfVectorizer on the new string column
tf.fit(df_cbf['genres_string'])

# You can keep the original 'genres' list column or drop it if not needed later
# df_cbf = df_cbf.drop('genres', axis=1)

tf.get_feature_names_out()

df_cbf

"""### Menghapus Kolom `genres`"""

df_cbf = df_cbf.drop('genres', axis=1)
df_cbf.head()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(df_cbf['genres_string'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# Membuat dataframe untuk melihat tf-idf matrix
pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=df_cbf.title
).sample(21, axis=1).sample(10, axis=0)

"""### Cosine Similarity"""

from sklearn.metrics.pairwise import cosine_similarity

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim # Display the result

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama title
cosine_sim_df = pd.DataFrame(cosine_sim, index=df_cbf['title'], columns=df_cbf['title'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap title
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""### Mendapatkan Rekomendasi"""

def movie_recommendations(movie_title, similarity_data=cosine_sim_df, items=df_cbf[['title', 'genres_string']], k=10):
    """
    Rekomendasi Film berdasarkan kemiripan dataframe
    Parameter:
    ---
    movie_title : tipe data string (str)
                Nama Film (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan judul film sebagai
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---
    Pada index ini, kita mengambil k dengan nilai similarity terbesar
    pada index matrix yang diberikan (i).
    """
    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,movie_title].to_numpy().argpartition(
        range(-1, -k, -1))
    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    # Drop movie_title agar judul film yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(movie_title, errors='ignore')
    return pd.DataFrame(closest).merge(items).head(k)

"""### Memasukkan nama `title`"""

df_cbf[df_cbf.title.eq('Clueless 1995')]

"""### Menampilkan rekomendasi yang diberikan berdasarkan `genres`"""

movie_recommendations('Clueless 1995')

"""## Collaborative Filtering"""

df_cf = df_merge[['userId', 'movieId', 'rating']]
df_cf

"""## Data Preparation (Collaborative Filtering)

### Encoding Data
"""

# Mengubah userId menjadi list tanpa nilai yang sama
user_ids = df_cf['userId'].unique().tolist()
print('list userId: ', user_ids)
# Melakukan encoding userId
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userId : ', user_to_user_encoded)
# Melakukan proses encoding angka ke ke userId
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userId: ', user_encoded_to_user)

# Mengubah movieId menjadi list tanpa nilai yang sama
movie_ids = df_cf['movieId'].unique().tolist()
print('list movieId: ', movie_ids)
# Melakukan encoding movieId
movie_to_movie_encoded = {x: i for i, x in enumerate(movie_ids)}
print('encoded movieId : ', movie_to_movie_encoded)
# Melakukan proses encoding angka ke ke movieId
movie_encoded_to_movie = {i: x for i, x in enumerate(movie_ids)}
print('encoded angka ke movieId: ', movie_encoded_to_movie)

# Mapping userId ke dataframe
df_cf['user'] = df_cf['userId'].map(user_to_user_encoded)
# Mapping movieId ke dataframe
df_cf['movie'] = df_cf['movieId'].map(movie_to_movie_encoded)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(f'Jumlah user: {num_users}')

# Mendapatkan jumlah movie
num_movies = len(movie_encoded_to_movie)
print(f'Jumlah movie: {num_movies}')

# Mengubah rating menjadi nilai float
df_cf['rating'] = df_cf['rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(df_cf['rating'])

# Nilai maksimal rating
max_rating = max(df_cf['rating'])

print('Number of User: {}, Number of Movie: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_movies, min_rating, max_rating
))

"""## Mengacak Data

Acak datanya terlebih dahulu agar distribusinya menjadi random. Proses ini bertujuan untuk mengacak urutan data agar distribusinya menjadi lebih acak dan tidak mengikuti pola tertentu
"""

df_cf = df_cf.sample(frac=1, random_state=42)
df_cf

"""## Membagi Data untuk Training dan Validasi"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
X = df_cf[['user', 'movie']].values
y = MinMaxScaler().fit_transform(df_cf[['rating']])

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.2, random_state=42)

print(f'Banyaknya data pada X_train: {len(X_train)}')
print(f'Banyaknya data pada y_train: {len(y_train)}')
print(f'Banyaknya data pada X_val: {len(X_val)}')
print(f'Banyaknya data pada y_val: {len(y_val)}')

"""## Modeling Collaborative Filtering"""

import tensorflow as tf
from tensorflow import keras

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_resto, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_resto = num_resto
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.resto_embedding = layers.Embedding( # layer embeddings resto
        num_resto,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.resto_bias = layers.Embedding(num_resto, 1) # layer embedding resto bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    resto_vector = self.resto_embedding(inputs[:, 1]) # memanggil layer embedding 3
    resto_bias = self.resto_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_resto = tf.tensordot(user_vector, resto_vector, 2)

    x = dot_user_resto + user_bias + resto_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_movies, 20) # inisialisasi model
# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""## Evaluation"""

history = model.fit(
    x = X_train,
    y = y_train,
    batch_size = 8,
    epochs = 20,
    validation_data = (X_val, y_val)
)

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""### Mendapatkan Rekomendasi"""

movies_df = movies
df = pd.read_csv('/content/movies-and-ratings-for-recommendation-system/ratings.csv')

# Mengambil sample user
user_id = df['userId'].sample(1).iloc[0]
movies_watched_by_user = df[df['userId'] == user_id]

# Mengambil data film yang belum ditonton user
movies_not_watched = movies_df[~movies_df['movieId'].isin(movies_watched_by_user.movieId.values)]['movieId']
movies_not_watched = list(
    set(movies_not_watched)
    .intersection(set(movie_to_movie_encoded.keys()))
)
movies_not_watched = [[movie_to_movie_encoded.get(x)] for x in movies_not_watched]
user_encoder = user_to_user_encoded.get(user_id)
user_movie_array = np.hstack(
    ([[user_encoder]] * len(movies_not_watched), movies_not_watched)
)

movies_df

"""### Menampilkan rekomendasi"""

# Memprediksi rating
ratings = model.predict(user_movie_array).flatten()

# Mendapatkan top N rekomendasi
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_movie_ids = [
    movie_encoded_to_movie.get(movies_not_watched[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Movie with high ratings from user')
print('----' * 8)

# Top 5 film yang ditonton user
top_movies_user = (
    movies_watched_by_user.sort_values(by='rating', ascending=False)
    .head(5).movieId.values
)
movie_df_rows = movies_df[movies_df['movieId'].isin(top_movies_user)]
for row in movie_df_rows.itertuples():
  print(f"{row.title} (Genres: {row.genres})")

print('----' * 8)
print('Top 10 movie recommendation')
print('----' * 8)

recommended_movies = movies_df[movies_df['movieId'].isin(recommended_movie_ids)]
for row in recommended_movies.itertuples():
  print(f"{row.title} (Genres: {row.genres})")